{"paragraphs":[{"text":"%spark2.spark\n\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.{Dataset, SparkSession}\n\nval schema: StructType = new StructType(Array(StructField(\"date\", StringType, false),\n    StructField(\"game_size\", IntegerType, false),\n    StructField(\"match_id\",StringType, false),\n    StructField(\"match_mode\", StringType, false),\n    StructField(\"party_size\", IntegerType, false),\n    StructField(\"player_assists\", IntegerType, false),\n    StructField(\"player_dbno\",IntegerType,false),\n    StructField(\"player_dist_ride\",DoubleType,false),\n    StructField(\"player_dist_walk\",DoubleType,false),\n    StructField(\"player_dmg\",IntegerType,false),\n    StructField(\"player_kills\",IntegerType,false),\n    StructField(\"player_name\",StringType,false),\n    StructField(\"player_survive_time\",DoubleType,false),\n    StructField(\"team_id\",IntegerType,false),\n    StructField(\"team_placement\",IntegerType,false))\n  )\n\ncase class Player (game_size: Int, match_id: String, match_mode: String, party_size: Int,\n                   player_assists: Int, player_dbno: Int, player_dist_ride: Double, player_dist_walk: Double,\n                   player_dmg: Int, player_kills: Int, player_name: String, player_survive_time: Double,\n                   team_id: Int, team_placement: Int)\n                   \ndef ingest(spark: SparkSession, srcDir: String, schema: StructType): Dataset[Player]  = {\n\n    import spark.implicits._\n\n    spark.read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"false\")\n      .schema(schema)\n      .format(\"csv\")\n      .load(srcDir)\n      .as[Player]\n  }\n\nval path = \"s3a://csye7200.bucket.forfinal/testData/*\"\n\nval ds = ingest(spark, path, schema)","user":"admin","dateUpdated":"2018-04-21T06:00:59+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.types._\n\nimport org.apache.spark.sql.{Dataset, SparkSession}\n\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(date,StringType,false), StructField(game_size,IntegerType,false), StructField(match_id,StringType,false), StructField(match_mode,StringType,false), StructField(party_size,IntegerType,false), StructField(player_assists,IntegerType,false), StructField(player_dbno,IntegerType,false), StructField(player_dist_ride,DoubleType,false), StructField(player_dist_walk,DoubleType,false), StructField(player_dmg,IntegerType,false), StructField(player_kills,IntegerType,false), StructField(player_name,StringType,false), StructField(player_survive_time,DoubleType,false), StructField(team_id,IntegerType,false), StructField(team_placement,IntegerType,false))\n\ndefined class Player\n\ningest: (spark: org.apache.spark.sql.SparkSession, srcDir: String, schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.Dataset[Player]\n\npath: String = s3a://csye7200.bucket.forfinal/testData/*\n\nds: org.apache.spark.sql.Dataset[Player] = [date: string, game_size: int ... 13 more fields]\n"}]},"apps":[],"jobName":"paragraph_1524287400890_-425577132","id":"20180421-051000_2008985291","dateCreated":"2018-04-21T05:10:00+0000","dateStarted":"2018-04-21T06:00:59+0000","dateFinished":"2018-04-21T06:01:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2725"},{"text":"%spark2.spark\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel\nimport org.apache.spark.ml.clustering.KMeansModel\nobject models {\n    val nnModelForSolo = MultilayerPerceptronClassificationModel.load(\"s3a://csye7200.bucket.forfinal/Models/nnClassifierForSolo\")\n    val nnModelForDuo = MultilayerPerceptronClassificationModel.load(\"s3a://csye7200.bucket.forfinal/Models/nnClassifierForDuo\")\n    val nnModelForSquad = MultilayerPerceptronClassificationModel.load(\"s3a://csye7200.bucket.forfinal/Models/nnClassifierForSquad\")\n\n    val clusteringByDistForSolo = KMeansModel.load(\"s3a://csye7200.bucket.forfinal/Models/ClusteringByDistanceForSolo\")\n    val clusteringByDistForDuo = KMeansModel.load(\"s3a://csye7200.bucket.forfinal/Models/ClusteringByDistanceForDuo\")\n    val clusteringByDistForSquad = KMeansModel.load(\"s3a://csye7200.bucket.forfinal/Models/ClusteringByDistanceForSquad\")\n\n    val clusteringByBattleForSolo = KMeansModel.load(\"s3a://csye7200.bucket.forfinal/Models/ClusteringByBattleForSolo\")\n    val clusteringByBattleForDuo = KMeansModel.load(\"s3a://csye7200.bucket.forfinal/Models/ClusteringByBattleForDuo\")\n    val clusteringByBattleForSquad = KMeansModel.load(\"s3a://csye7200.bucket.forfinal/Models/ClusteringByBattleForSquad\")\n  }","user":"admin","dateUpdated":"2018-04-21T06:01:42+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel\n\nimport org.apache.spark.ml.clustering.KMeansModel\n\ndefined object models\n"}]},"apps":[],"jobName":"paragraph_1524287474270_-964535365","id":"20180421-051114_1778032762","dateCreated":"2018-04-21T05:11:14+0000","dateStarted":"2018-04-21T06:01:42+0000","dateFinished":"2018-04-21T06:01:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2726"},{"text":"%spark2.spark\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.ml.feature.{StandardScaler, VectorAssembler, Normalizer}\n\ndef filterPlayers(ds: Dataset[Player]): Dataset[Player] = {\n    val filterdPlayers = ds.filter(d => (d.player_dist_ride != 0 || d.player_dist_walk != 0)\n      && d.player_survive_time <= 2400\n      && d.team_placement > 0)\n    filterdPlayers\n  }\n\ndef dropCols(df: DataFrame): DataFrame = {\n    df.drop(\"date\",\"game_size\",\"match_id\",\"match_mode\",\"party_size\",\"player_name\",\"team_id\")\n  }\n\nobject Clustering {\n  \n  def createDfWithFeature(ds: Dataset[Player],inputCols: Array[String]): DataFrame = {\n    val vecAss = new VectorAssembler().setInputCols(inputCols).setOutputCol(\"unscaled_features\")\n    val assembledDf = vecAss.transform(ds)\n    val scaler = new StandardScaler().setInputCol(\"unscaled_features\").setOutputCol(\"features\")\n    val scalerModel = scaler.fit(assembledDf)\n    val scaledDf = scalerModel.transform(assembledDf)\n    dropCols(scaledDf)\n  }\n}\n\n\nobject Classification {\n  def createDfWithFeature(ds: Dataset[Player]): DataFrame ={\n    val isWinnerUdf = udf((placement: Int) => placement match {\n      case 1 => 1\n      case _ => 0\n    })\n    val colArray = Array(\"player_assists\", \"player_dbno\", \"player_dist_ride\", \"player_dist_walk\", \"player_dmg\", \"player_kills\", \"player_survive_time\")\n    val vecAss = new VectorAssembler().setInputCols(colArray).setOutputCol(\"feature_unnormalized\")\n    val normalizer = new Normalizer().setInputCol(\"feature_unnormalized\").setOutputCol(\"features\")\n    val df_temp_1 = vecAss.transform(ds)\n    val df_temp_2 = normalizer.transform(df_temp_1)\n    df_temp_2.withColumn(\"survived\", isWinnerUdf(df_temp_2(\"team_placement\")))\n  }\n}\n\n  val filteredDS = filterPlayers(ds)\n  val soloPlayers = filteredDS.filter(d => d.party_size == 1).cache()\n  val duoPlayers = filteredDS.filter(d => d.party_size == 2).cache()\n  val squadPlayers = filteredDS.filter(d => d.party_size == 4).cache()","user":"admin","dateUpdated":"2018-04-21T06:03:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.DataFrame\n\nimport org.apache.spark.ml.feature.{StandardScaler, VectorAssembler, Normalizer}\n\nfilterPlayers: (ds: org.apache.spark.sql.Dataset[Player])org.apache.spark.sql.Dataset[Player]\n\ndropCols: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n\ndefined object Clustering\n\ndefined object Classification\n\nfilteredDS: org.apache.spark.sql.Dataset[Player] = [date: string, game_size: int ... 13 more fields]\n\nsoloPlayers: org.apache.spark.sql.Dataset[Player] = [date: string, game_size: int ... 13 more fields]\n\nduoPlayers: org.apache.spark.sql.Dataset[Player] = [date: string, game_size: int ... 13 more fields]\n\nsquadPlayers: org.apache.spark.sql.Dataset[Player] = [date: string, game_size: int ... 13 more fields]\n"}]},"apps":[],"jobName":"paragraph_1524287515555_614150669","id":"20180421-051155_683852006","dateCreated":"2018-04-21T05:11:55+0000","dateStarted":"2018-04-21T06:03:35+0000","dateFinished":"2018-04-21T06:03:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2727"},{"text":"%spark2.spark\n\n    val inputCols = Array(\"player_dist_ride\", \"player_dist_walk\")\n    //Get the dateframe with features column\n    val soloDF_clustering_dist = Clustering.createDfWithFeature(soloPlayers, inputCols)\n    val duoDF_clustering_dist = Clustering.createDfWithFeature(duoPlayers, inputCols)\n    val squadDF_clustering_dist = Clustering.createDfWithFeature(squadPlayers, inputCols)\n    //Get the prediction result of clustering by distance\n    val solo_predicted_by_dist = models.clusteringByDistForSolo.transform(soloDF_clustering_dist).cache\n    val duo_predicted_by_dist = models.clusteringByDistForDuo.transform(duoDF_clustering_dist).cache\n    val squad_predicted_by_dist = models.clusteringByDistForSquad.transform(squadDF_clustering_dist).cache","user":"admin","dateUpdated":"2018-04-21T06:04:18+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninputCols: Array[String] = Array(player_dist_ride, player_dist_walk)\n\nsoloDF_clustering_dist: org.apache.spark.sql.DataFrame = [player_assists: int, player_dbno: int ... 8 more fields]\n\nduoDF_clustering_dist: org.apache.spark.sql.DataFrame = [player_assists: int, player_dbno: int ... 8 more fields]\n\nsquadDF_clustering_dist: org.apache.spark.sql.DataFrame = [player_assists: int, player_dbno: int ... 8 more fields]\n\nsolo_predicted_by_dist: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [player_assists: int, player_dbno: int ... 9 more fields]\n\nduo_predicted_by_dist: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [player_assists: int, player_dbno: int ... 9 more fields]\n\nsquad_predicted_by_dist: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [player_assists: int, player_dbno: int ... 9 more fields]\n"}]},"apps":[],"jobName":"paragraph_1524288220667_1605061856","id":"20180421-052340_1924022633","dateCreated":"2018-04-21T05:23:40+0000","dateStarted":"2018-04-21T06:04:18+0000","dateFinished":"2018-04-21T06:11:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2728"},{"text":"%spark2.spark\n\n     val inputCols1 = Array(\"player_dmg\", \"player_kills\")\n     val inputCols2 = Array(\"player_assists\", \"player_dbno\", \"player_dmg\", \"player_kills\")\n     //Get the dateframe with features column\n     val soloDF_clustering_battle = Clustering.createDfWithFeature(soloPlayers, inputCols1)\n     val duoDF_clustering_battle = Clustering.createDfWithFeature(duoPlayers, inputCols2)\n     val squadDF_clustering_battle = Clustering.createDfWithFeature(squadPlayers, inputCols2)\n     //Get the prediction result of clustering by battle\n     val solo_predicted_by_battle = models.clusteringByBattleForSolo.transform(soloDF_clustering_battle)\n     val duo_predicted_by_battle = models.clusteringByBattleForDuo.transform(duoDF_clustering_battle)\n     val squad_predicted_by_battle = models.clusteringByBattleForSquad.transform(squadDF_clustering_battle)","user":"admin","dateUpdated":"2018-04-21T06:12:21+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninputCols1: Array[String] = Array(player_dmg, player_kills)\n\ninputCols2: Array[String] = Array(player_assists, player_dbno, player_dmg, player_kills)\n\nsoloDF_clustering_battle: org.apache.spark.sql.DataFrame = [player_assists: int, player_dbno: int ... 8 more fields]\n\nduoDF_clustering_battle: org.apache.spark.sql.DataFrame = [player_assists: int, player_dbno: int ... 8 more fields]\n\nsquadDF_clustering_battle: org.apache.spark.sql.DataFrame = [player_assists: int, player_dbno: int ... 8 more fields]\n\nsolo_predicted_by_battle: org.apache.spark.sql.DataFrame = [player_assists: int, player_dbno: int ... 9 more fields]\n\nduo_predicted_by_battle: org.apache.spark.sql.DataFrame = [player_assists: int, player_dbno: int ... 9 more fields]\n\nsquad_predicted_by_battle: org.apache.spark.sql.DataFrame = [player_assists: int, player_dbno: int ... 9 more fields]\n"}]},"apps":[],"jobName":"paragraph_1524288673147_1755575297","id":"20180421-053113_390535834","dateCreated":"2018-04-21T05:31:13+0000","dateStarted":"2018-04-21T06:12:21+0000","dateFinished":"2018-04-21T06:12:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2729"},{"text":"%spark2.spark\n\n    val soloDF_classification = Classification.createDfWithFeature(soloPlayers)\n    val duoDF_classification = Classification.createDfWithFeature(duoPlayers)\n    val squadDF_classification = Classification.createDfWithFeature(squadPlayers)\n\n    val solo_classification = models.nnModelForSolo.transform(soloDF_classification)\n    val duo_classification = models.nnModelForDuo.transform(duoDF_classification)\n    val squad_classification = models.nnModelForSquad.transform(squadDF_classification)","user":"admin","dateUpdated":"2018-04-21T06:14:25+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524290099460_-2069223386","id":"20180421-055459_1806413159","dateCreated":"2018-04-21T05:54:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2730","dateFinished":"2018-04-21T06:14:25+0000","dateStarted":"2018-04-21T06:14:25+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nsoloDF_classification: org.apache.spark.sql.DataFrame = [date: string, game_size: int ... 16 more fields]\n\nduoDF_classification: org.apache.spark.sql.DataFrame = [date: string, game_size: int ... 16 more fields]\n\nsquadDF_classification: org.apache.spark.sql.DataFrame = [date: string, game_size: int ... 16 more fields]\n\nsolo_classification: org.apache.spark.sql.DataFrame = [date: string, game_size: int ... 17 more fields]\n\nduo_classification: org.apache.spark.sql.DataFrame = [date: string, game_size: int ... 17 more fields]\n\nsquad_classification: org.apache.spark.sql.DataFrame = [date: string, game_size: int ... 17 more fields]\n"}]}},{"text":"%spark2.spark\n\nsolo_classification.where(\"survived == 1 and prediction == 1\").count\nsolo_classification.where(\"prediction == 1\").count","user":"admin","dateUpdated":"2018-04-21T06:16:59+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524291265102_-1892140369","id":"20180421-061425_1720309957","dateCreated":"2018-04-21T06:14:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3487","dateFinished":"2018-04-21T06:17:10+0000","dateStarted":"2018-04-21T06:16:59+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres28: Long = 1451\n\nres29: Long = 2251\n"}]}},{"text":"%spark2.spark\n","user":"admin","dateUpdated":"2018-04-21T06:15:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524291313614_536272095","id":"20180421-061513_30013709","dateCreated":"2018-04-21T06:15:13+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3574"}],"name":"/csye7200/Integration_NoteBook","id":"2DC3XKYET","angularObjects":{"2DBSU32NV:shared_process":[],"2DCZCCVM8:shared_process":[],"2DBFN3YKX:shared_process":[],"2DCWPAYPU:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2DCCXWCG2:shared_process":[],"2DAUQ34KJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}
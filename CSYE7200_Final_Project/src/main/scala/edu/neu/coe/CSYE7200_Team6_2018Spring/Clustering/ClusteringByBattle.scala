package edu.neu.coe.CSYE7200_Team6_2018Spring.Clustering


import edu.neu.coe.CSYE7200_Team6_2018Spring.Ingest.Player
import edu.neu.coe.CSYE7200_Team6_2018Spring.Ingest.Player.IngestPlayer
import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
import org.apache.spark.sql.{DataFrame, Dataset}


object ClusteringByBattle extends Clustering{

  val inputCols1 = Array("player_dmg","player_kills")
  val inputCols2 = Array("player_assists","player_dbno","player_dmg","player_kills")

  override def clusteringHelper(dsSeprated: Dataset[Player]): KMeansModel = {
    val k = dsSeprated.head().party_size match {
      // Numbers of clusters were generated by using elbow method called determinK in trait clustering
      case 1 => 5
      case 2 => 5
      case 4 => 4
      case _ => 0
    }
    val kmeans = new KMeans().setK(k).setSeed(1L)
    if(dsSeprated.head.party_size == 1) {
      val fitDf = createDfWithFeature(dsSeprated,inputCols1)
      fitDf.cache()
      kmeans.fit(fitDf)
    }else {
      val fitDf = createDfWithFeature(dsSeprated,inputCols2)
      fitDf.cache()
      kmeans.fit(fitDf)
    }
  }

  override def dropCols(df: DataFrame): DataFrame = {
    val party_size = df.select("party_size").take(1).map(_(0)).toList.head
    if(party_size == 1) {
      df.drop("date","game_size","match_id","match_mode","party_size","player_assists","player_dbno","player_dist_ride","player_dist_walk","player_name","player_survive_time","team_id")
    } else{
      df.drop("date","game_size","match_id","match_mode","party_size","player_dist_ride","player_dist_walk","player_name","player_survive_time","team_id")
    }
  }

  // the entrance of calling clustering
  def main(args: Array[String]): Unit = {
    val schema = Player.schema
    implicit val spark = Player.spark
    val dataset = IngestPlayer.ingest("sample.csv", schema)
    ClusteringByDistance.clustering(dataset)
  }

}

package edu.neu.coe.CSYE7200_Team6_2018Spring.Clustering

import edu.neu.coe.CSYE7200_Team6_2018Spring.Clustering.ClusteringByBattle.{inputCols1, inputCols2}
import edu.neu.coe.CSYE7200_Team6_2018Spring.Ingest.Player
import edu.neu.coe.CSYE7200_Team6_2018Spring.Ingest.Player.IngestPlayer
import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
import org.apache.spark.ml.linalg.{DenseVector, SparseVector}
import org.apache.spark.sql.{DataFrame, Dataset}
import org.scalatest.{FlatSpec, Matchers}

class ClusteringSpec extends FlatSpec with Matchers {

  val schema = Player.schema
  implicit val spark = Player.spark
  val dataset = IngestPlayer.ingest("sample.csv", schema)
  val filteredDS = IngestPlayer.filterPlayers(dataset)

  class TestClustering extends Clustering {
    override def clusteringHelper(dsSeprated: Dataset[Player]): KMeansModel = {
      val k = dsSeprated.head().party_size match {
        // Numbers of clusters were generated by using elbow method called determinK in trait clustering
        case 1 => 5
        case 2 => 5
        case 4 => 4
        case _ => 0
      }
      val kmeans = new KMeans().setK(k).setSeed(1L)
      if (dsSeprated.head.party_size == 1) {
        val fitDf = createDfWithFeature(dsSeprated, inputCols1)
        fitDf.cache()
        kmeans.fit(fitDf)
      } else {
        val fitDf = createDfWithFeature(dsSeprated, inputCols2)
        fitDf.cache()
        kmeans.fit(fitDf)
      }
    }

    override def dropCols(df: DataFrame): DataFrame = {
      df.drop("date", "game_size", "match_id", "match_mode", "player_assists", "player_dbno", "player_name", "player_survive_time", "team_id")
    }
  }

  behavior of "Clustering"

  it should "work for creating the input column of clustering" in {
    val inputCols = Array("player_dist_ride", "player_dist_walk", "player_dmg", "player_kills")
    object TestClustering extends TestClustering
    val scaledDf = TestClustering.createDfWithFeature(filteredDS, inputCols)
    scaledDf.columns.length shouldBe 8
    val rowtypes = scaledDf.select("unscaled_features").collect().map(_(0))
    val rows = for(row <- rowtypes;if row.isInstanceOf[DenseVector] || row.isInstanceOf[SparseVector]) yield row
    rows should have length 9779
  }
}

